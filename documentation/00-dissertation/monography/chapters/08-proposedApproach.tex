\chapter{Abordagem Proposta} \label{chap:propApproach}
	\section{A Base de Sinais de Voz}
	    \subsection{Coleta dos Sinais}
		\par Para a realização desta pesquisa, coletou-se uma série de vozes nos arredores do Instituto de Biociências, Letras e Ciências Exatas da UNESP em São José do Rio Preto, no estado de São Paulo. Foram coletadas amostras de 21 indivíduos, das quais 20 foram usadas já que, em um dos casos, não foi possível coletar todos os dados necessários. Tais gravações se constituem de dígitos em um intervalo de 0 a 9 falados tanto em língua Inglesa quanto na Portuguesa. Os locutores foram escolhidos de acordo com seu sexo e idade de forma que a amostra estudada tenha uma abrangência que cubra desde crianças em época pré-escolar até adultos entre 50 e 60 anos dos sexos masculino e feminino.
					
		\par As gravações foram realizadas em ambientes distintos com diferentes níveis de ruído ao fundo, garantindo uma boa variabilidade de interferências corriqueiras, caracterizando casos reais. Foram usados arquivos no formato \textit{wav} sem compressão. A taxa de amostragem escolhida foi de 44100Hz, que permite, segundo o teorema de Nyquist, que seja realizada a quantização de frequências de até 22050Hz, com quantização de 16 bits.
		
		\par Coletados os sinais, os dígitos pronunciados nos mesmos foram separados um-a-um usando uma ferramenta desenvolvida para esse fim, resultando em um total de 410 sinais de voz de diferentes durações temporais que foram rotulados como ``genuínos''. Para cada um deles, foi criado um sinal ``espelho'',  regravado por um segundo dispositivo de gravação diferente do original, caracterizando os 410 sinais rotulados como ``regravados''.

	    \subsection{Organização da Base de Sinais}
		\par A organização da base de dados se deu por tipo, isto é, genuíno ou regravado, idioma, dígito ditado e interlocutor considerado. Foi criada uma estrutura hierárquica de diretórios de forma a permitir acesso fácil e intuitivo a cada um dos arquivos \textit{wav}, seja por vias automatizadas ou não. Os arquivos regravados residem no diretório ``playback'', enquanto que os genuínos se encontram no diretório ``live ''.	Essa organização está ilustrada nas Figuras \ref{fig:directorystructlevel01}, \ref{fig:directorystructlevel02} e \ref{fig:directorystructlevel03}.
		
		\par Para facilitar a automatização do processamento, foram criados três arquivos de texto:
		\begin{itemize}
			\item \textit{\textbf{dataSurvey.txt}}: contêm os dados de idade e sexo e nome do arquivo gerado para cada entrevistado;
			\item \textit{\textbf{inputListLive.txt}}: uma lista de caminhos para todos os arquivo não regravados;
			\item \textit{\textbf{inputListSpoofing.txt}}: apresenta uma listagem dos caminhos para todos os arquivos regravados.
		\end{itemize}
	
		\par Apenas para ilustrar, o conteúdo do diretório \textbf{``separated \textfractionsolidus live \textfractionsolidus en\_US \textfractionsolidus 0''} se constitui de vários arquivos do tipo \textit{wav}, cada um identificando o locutor ao qual pertence como exibido na Figura \ref{fig:directorystructlevel03}.

		\begin{figure}[ht]
			\centering
			\subfloat[0.3\textwidth][Base em nível 1]{
				\includegraphics{images/directoryStructLevel01}
				\label{fig:directorystructlevel01}
			}
			\subfloat[0.3\textwidth][Base em nível 2]{
				\includegraphics{images/directoryStructLevel02}
				\label{fig:directorystructlevel02}
			}
			\subfloat[0.3\textwidth][Base em nível 3]{
				\includegraphics{images/directoryStructLevel03}
				\label{fig:directorystructlevel03}
			}
			\caption{Organização da base de dados}
			\label{fig:directorystructlevel010203}
		\end{figure}

	\section{Estrutura da Estratégia Proposta}
		\par A estratégia proposta para diferenciar sinais de voz genuínos daqueles regravados deu-se conforme ilustrado na Figura \ref{fig_arq}. Particularmente, a metodologia consiste na obtenção dos dados brutos de todos os 410 + 410 = 820 sinais de voz genuínos e regravados, seguida da conversão de cada um deles para um vetor de características correspondente, conforme explicado adiante. Na sequência, os melhores sub-conjuntos de características foram escolhidos com base na Engenharia Paraconsistente. Prosseguindo, separações aleatórias entre os vetores, com proporções diversas, foram realizadas para isolar aqueles destinados ao treinamento ou modelagem do classificador utilizado dos que foram destinados aos testes de classificação. Por fim, testes e resultados foram realizados, conforme descrito no Capítulo seguinte.
		
		\input{images/strategicStructure.tex}
		
		\par Conforme mencionado nos Capítulos anteriores, os vetores de características desta abordagem foram obtidos com base na Transformada \textit{Wavelet}, convertendo os sinais de voz do domínio do tempo para o domínio tempo-frequência. Particularmente, nos experimentos detalhados adiante, foram testados os seguintes filtros \textit{wavelet}: Haar, Daubechies de suportes 4 até 76, Symmlets com suportes 8, 16 e 32, Coiflets com suportes 6, 12, 18, 24 e 30, Beylkin com suporte 18 e, ainda, Vaidyanathan de suporte 24.

	\section{Procedimentos}
		\par Afim de garantir a comparação com outros trabalhos se fez necessário adotar várias formas de representar os resultados correspondentes para cada configuração experimental:
		\begin{itemize}
			\item Tabela de confusão.
			\item Acurácia e seu respectivo desvio padrão.
			\item EER (equal error rate).
		\end{itemize}
		
		\par No exemplo constituído pela tabela de confusão \ref{tab:confusionMatrixSample} as \textbf{linhas} representam as \textbf{classes estimadas} e as \textbf{colunas} as \textbf{classes verdadeiras}, sendo que:
		\begin{itemize}
			\item \textbf{VV}: Quantidade de itens verdadeiros classificados como tal.
			\item \textbf{VF}: Quantidade de itens falsos classificados como tal.
			\item \textbf{FF}: Quantidade de itens verdadeiros classificados como falsos.
			\item \textbf{FV}: Quantidade de itens falsos classificados como verdadeiros.
		\end{itemize} 
		\input{tables/results/confusionMatrices/confusionMatrixSample.tex}
		
		\par A acurácia usa os valores de \textit{VV}, \textit{VF} e a quantidade de elementos(\textit{n}) tal que a mesma pode ser calculada como mostrado na Equação \ref{eq:calculoDaAcuracia}.
		
		\begin{equation}
			acuracia = \dfrac{VV + VF}{n} \qquad.
			\label{eq:calculoDaAcuracia}
		\end{equation}

		\par Para o cálculo do EER \textit{FV} e \textit{FF} são utilizados \cite{ghazali2018recent} de acordo com a Equação \ref{eq:calculoDoEER}.
		
		\begin{equation}
			EER = \dfrac{FV}{FF} \qquad.
			\label{eq:calculoDoEER}
		\end{equation}
	
		\subsection{Procedimento 01: filtros \textit{wavelet}, escalas Bark e Mel}
		\label{chap:propApproach:sec:Experimento01}
		\par O objetivo deste procedimento é o de verificar, segundo a Engenharia Paraconsistente, qual das combinações entre as escalas BARK ou MEL e as várias \textit{wavelets} consideradas geram os vetores de características mais propícios, ou seja, que atraem o ponto $(G_1,G_2)$ para uma posição mais próxima do vértice $(1,0)$ do plano paraconsistente, conforme mencionado no Capítulo anterior. 
				
		\par As transformações \textit{wavelet-packet} foram realizadas, com os diversos filtros mencionados, até  o máximo nível possível, implicando máxima resolução em frequência, para que, após isso, as amostras dos sinais transformados fossem agrupadas visando corresponder aos intervalos espectrais definidos nas escalas BARK e MEL. Naquela escala, os vetores de características foram constituídos de 24 coeficientes. Diferentemente, nesta escala, os vetores de características foram formados com 13 coeficientes devido a derivação do sinal ao final do processo de geração. O algoritmo \ref{lst:experiment01Algo} contém a descrição de tais procedimentos.
			
		\input{codeListings/experiment01Algo.tex}

		\par Registre-se que, antes da aplicação da transformada \textit{wavelet-packet}, foi necessário redimensionar os sinais para que houvesse um comprimento correspondente a uma potência de 2, como indicado na equação \ref{eq:optimalSize}. Isso é necessário para que não haja perdas de trechos de voz ao final da transformação pois a transformada \textit{wavelet} realiza o \textit{downsampling} por 2, ou seja, em cada nível de decomposição o tamanho do vetor do sinal é diminuído pela metade. Caso haja um comprimento diferente do citado, em alguma parte do processo a divisão não será inteira fazendo com que algumas amostras dos sinais sejam perdidas.
				
		\par Para ajustar o tamanho do sinal de voz sob análise, foi usada a Equação \ref{eq:optimalSize}, na qual \textit{\textbf{proxInt}} é uma função que retorna o próximo número inteiro do argumento real. Por exemplo, $proxInt(1,5) = 2$.

		\begin{equation}
					tamanhoOtimo=2^{proxInt(\log_{2}tamanho)}
					\label{eq:optimalSize}
		\end{equation} 
		
		\par Após o redimensionamento do tamanho do sinal, o nível máximo de transformações é dado pela Equação \ref{eq:maxWaveletTransf}. 
				
		\begin{equation}
					maxTrans=\log_{2}(tamanho) \qquad.
					\label{eq:maxWaveletTransf}
		\end{equation}
		
		\subsection{Procedimento 02 - classificações baseadas em distâncias}
		\label{chap:propApproach:sec:Experimento02}
		\par O objetivo deste procedimento é verificar, considerando as melhores combinações descobertas pelo procedimento anterior, a acurácia de classificadores \textit{pattern-matching} por distâncias Euclidiana e Manhattan. Nesta fase, os vetores de características gerados pelo procedimento 01 são fornecidos aos classificadores para as mensurações devidas.
				
		\par Objetivando avaliar o comportamento dos classificadores com proporções múltiplas de 10\% da base de dados para modelagem, até o limite de 50\%, e o restante para testes, definiu-se que, para cada proporção, o sorteio aleatório para escolha dos vetores de características seria executado $n=t \cdot \frac{t+1}{2}$ vezes, sendo $t$ o número máximo de testes que podem ser realizados com uma certa porcentagem dos vetores. Em cada uma dessas execuções, a ordem dos vetores dentro das amostras, regravados e genuínos, foi permutada aleatoriamente.
				
		\par Para cada porcentagem foram coletadas as melhores e as piores acurácias assim como suas respectivas matrizes de confusão e assim calculadas suas \textit{EERs} conforme consta no Capítulo seguinte. No algoritmo \ref{lst:experiment02Algo}, os passos estão detalhados.
		
		\par Essencialmente, este procedimento 02 consiste em mensurar as distâncias entre cada vetor de características isolado para testes em relação à cada um dos vetores de características isolados para a modelagem, selecionando-se a menor delas. Em seguida, classifica-se o vetor de características que pertence aos testes e está sob análise, como pertencente à uma das classes dos vetores de modelagem selecionados.   
								
		\input{codeListings/experiment02Algo.tex}

		\subsection{Procedimento 03 - classificações baseadas na SVM}
		\label{chap:propApproach:sec:Experimento03}
		\par Considerando as melhores combinações descobertas durante o procedimento 01, esta etapa visa medir a acurácia de uma SVM na separação das classes. O referido classificador foi escolhido pois estudos anteriores comprovam a sua eficácia para classificação binária \cite{bennett2000support}. 
		
		\par Particularmente, a estrutura da SVM utilizada foi definida da seguinte forma e de acordo com a Figura \ref{fig:3layersSVM}: 
		\begin{itemize}
		\item{}três camadas, sendo a primeira, isto é, de entrada, com elementos passivos, a segunda com elementos ativos não-lineares de núcleos Gaussianos e a terceira, isto é, a de saída, com um elemento ativo linear; 
		\item{}inexistem pesos entre a camada de entrada e a camada intermediária, implicando que a saída de cada elemento da camada de entrada conecta-se com todas as entradas de cada elemento da camada intermediária, mantendo incólumes os valores propagados;
		\item{}a saída de cada elemento da camada intermediária conecta-se com o único elemento da camada de saída por meio dos pesos $p_0, p_1, .... p_{X-1}$;
		\item{}o valor de saída consiste na combinação linear dos pesos com os valores recebidos como entrada da camada de saída, isto é, os valores de saída da camada intermediária.
		\end{itemize}
		
		\input{images/3layersSVM.tex}

		\par Foram utilizados, na camada de entrada, um número de elementos igual a dimensão do vetor de características sendo considerado. Na camada intermediária, o número de elementos ativos não-lineares foi igual ao número de casos de treinamento, visando facilitar o procedimento que, em tal caso, implica na solução direta de um sistema linear quadrado, isto é, possível e determinado \cite{poole2014linear}. 
		
		\par O treinamento da SVM consiste em, numa primeira etapa não-supervisionada, ajustar os parâmetros das funções Gaussianas da camada intermediária. Posteriormente, com base no sistema linear mencionado, os pesos foram encontrados com base em uma abordagem supervisionada, utilizando-se os rótulos -1 e 1 para os sinais regravados e genuínos, respectivamente.    
		
		\par Todos os arranjos para a seleção dos vetores de treinamento e testes, assim como demais detalhes,   são idênticos àqueles do procedimento 02 e encontram-se listados no algoritmo \ref{lst:experiment03Algo}. 
		
		\input{codeListings/experiment03Algo.tex}

        \par Os testes e resultados dos experimentos descritos neste Capítulo encontram-se no Capítulo seguinte. 